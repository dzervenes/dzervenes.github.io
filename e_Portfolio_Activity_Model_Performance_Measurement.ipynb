{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAh5dhi4X0+EBU3sfZCszA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzervenes/dzervenes.github.io/blob/master/e_Portfolio_Activity_Model_Performance_Measurement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, I will conduct experiments to explore how varying different parameters affects the AUC (Area Under the Curve) and R² (R-squared) error values. The objective is to analyse the relationship between parameter changes and the resulting performance metrics, providing insights into the model's behaviour and predictive capabilities."
      ],
      "metadata": {
        "id": "91Yz7AuO_ik6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline values\n",
        "\n",
        "Running the default model, without altering its parameters, yields the following baseline values:\n",
        "\n",
        "- AUC (Linear Model): 0.994767718408118\n",
        "\n",
        "- AUC (Multiclass Case): 0.9913333333333334\n",
        "- R²: 0.9486081370449679\n",
        "\n",
        "These baseline metrics will serve as a reference point for evaluating how changes to various parameters influence the model's performance. By comparing these values with those obtained from parameter tuning, we can gain insights into the model's sensitivity to different configurations."
      ],
      "metadata": {
        "id": "B4zulrnB8wfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularisation strength**\n",
        "\n",
        "In this section, I explore the effect of regularisation strength (C) on the model's performance, measured using the AUC (Area Under the Curve) metric for both binary and multiclass classification tasks.\n",
        "\n",
        "The regularisation strength is set to two different values: C = 0.001 and C = 1000.\n",
        "\n",
        "This analysis provides insights into how varying regularisation strength impacts the classifier's ability to generalise across different tasks."
      ],
      "metadata": {
        "id": "V01ookwcBmkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_binary, y_binary = load_breast_cancer(return_X_y=True)\n",
        "X_multi, y_multi = load_iris(return_X_y=True)\n",
        "\n",
        "# Testing with Small C = 0.001\n",
        "print(\"\\nTesting with C = 0.001\")\n",
        "clf_binary_small = LogisticRegression(C=0.001, solver=\"liblinear\")\n",
        "clf_binary_small.fit(X_binary, y_binary)\n",
        "auc_binary_small = roc_auc_score(y_binary, clf_binary_small.predict_proba(X_binary)[:, 1])\n",
        "print(f\"Binary Classification - AUC: {auc_binary_small:.4f}\")\n",
        "\n",
        "clf_multi_small = LogisticRegression(C=0.001, solver=\"liblinear\")\n",
        "clf_multi_small.fit(X_multi, y_multi)\n",
        "auc_multi_small = roc_auc_score(y_multi, clf_multi_small.predict_proba(X_multi), multi_class='ovr')\n",
        "print(f\"Multiclass Classification - AUC: {auc_multi_small:.4f}\")\n",
        "\n",
        "# Testing with Large C = 1000\n",
        "print(\"\\nTesting with C = 1000\")\n",
        "clf_binary_large = LogisticRegression(C=1000, solver=\"liblinear\")\n",
        "clf_binary_large.fit(X_binary, y_binary)\n",
        "auc_binary_large = roc_auc_score(y_binary, clf_binary_large.predict_proba(X_binary)[:, 1])\n",
        "print(f\"Binary Classification - AUC: {auc_binary_large:.4f}\")\n",
        "\n",
        "clf_multi_large = LogisticRegression(C=1000, solver=\"liblinear\")\n",
        "clf_multi_large.fit(X_multi, y_multi)\n",
        "auc_multi_large = roc_auc_score(y_multi, clf_multi_large.predict_proba(X_multi), multi_class='ovr')\n",
        "print(f\"Multiclass Classification - AUC: {auc_multi_large:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g6coSSGBnf3",
        "outputId": "6d47c154-5061-475a-fcce-1244031ae7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing with C = 0.001\n",
            "Binary Classification - AUC: 0.9751\n",
            "Multiclass Classification - AUC: 0.8351\n",
            "\n",
            "Testing with C = 1000\n",
            "Binary Classification - AUC: 0.9969\n",
            "Multiclass Classification - AUC: 0.9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "Comparison of Binary Classification AUC\n",
        "\n",
        "- Baseline AUC: 0.9948\n",
        "\n",
        "- C = 0.001: AUC = 0.9751\n",
        "\n",
        "- C = 1000: AUC = 0.9969\n",
        "\n",
        "Comparison of Multiclass Classification AUC\n",
        "\n",
        "- Baseline AUC : 0.9913\n",
        "- C = 0.001: AUC = 0.8351\n",
        "- C = 1000: AUC = 0.9956\n",
        "\n",
        "For C = 0.001 (strong regularisation), both binary and multiclass AUC values are lower than the baseline. This indicates that excessive regularisation limits the model's ability to capture the complexity of the data, leading to underperformance. On the other hand, for C = 1000 (weak regularisation), both AUC values exceed the baseline. This suggests that reducing regularisation enhances the model's predictive power by allowing it to better adapt to the data's complexity. Overall, this comparison highlights the trade-off between regularisation strength and model performance, demonstrating the importance of finding a careful balance to optimise results."
      ],
      "metadata": {
        "id": "mX0qoy5IFbiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solver**\n",
        "\n",
        "In this section, I decided to experiment with the lbfgs solver for logistic regression to evaluate its impact on the model's performance. The solver is known for its efficiency and suitability for both binary and multiclass classification tasks.\n",
        "\n",
        "However, during the experiments, the default number of iterations (max_iter=100) was insufficient for convergence, resulting in a ConvergenceWarning. To address this, I increased the number of iterations to 5000 to ensure proper convergence of the model."
      ],
      "metadata": {
        "id": "-QWDTLeKGjIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary Classification with lbfgs\n",
        "clf_binary_lbfgs = LogisticRegression(solver=\"lbfgs\", max_iter=5000).fit(X_binary, y_binary)\n",
        "auc_binary_lbfgs = roc_auc_score(y_binary, clf_binary_lbfgs.predict_proba(X_binary)[:, 1])\n",
        "print(f\"Binary Classification - AUC: {auc_binary_lbfgs:.4f}\")\n",
        "\n",
        "# Multiclass Classification with lbfgs (default One-vs-Rest)\n",
        "clf_multi_lbfgs = LogisticRegression(solver=\"lbfgs\", max_iter=5000).fit(X_multi, y_multi)\n",
        "auc_multi_lbfgs = roc_auc_score(y_multi, clf_multi_lbfgs.predict_proba(X_multi), multi_class=\"ovr\")\n",
        "print(f\"Multiclass Classification - AUC: {auc_multi_lbfgs:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_CWfE_VGlej",
        "outputId": "7d7481d8-7449-4613-9c44-2770edfb9d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Classification - AUC: 0.9947\n",
            "Multiclass Classification - AUC: 0.9983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "The experiment using the lbfgs solver with max_iter=5000 yielded the following AUC values:\n",
        "\n",
        "- Binary Classification - AUC: 0.9947\n",
        "- Multiclass Classification - AUC: 0.9983\n",
        "\n",
        "For binary classification, the lbfgs solver provides performance comparable to the baseline, maintaining a consistently high AUC value. In contrast, for multiclass classification, lbfgs demonstrates a clear improvement over the baseline by achieving a higher AUC value and potentially better generalisation."
      ],
      "metadata": {
        "id": "rjxu459tKMjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularisation Penalty**\n",
        "\n",
        "In this section, I explore two types of regularisation penalties, l1 and l2, to evaluate their impact on the performance of a logistic regression model for binary classification. Due to the computational intensity of l1 regularisation, the default number of iterations (max_iter=100) was insufficient for the solver to converge. To address this issue, I increased max_iter to 1000, providing the solver with additional iterations to optimise the model's coefficients effectively."
      ],
      "metadata": {
        "id": "YWxcPUdqMBAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing L1 Penalty with increased iterations\n",
        "clf_binary_l1 = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", max_iter=1000).fit(X_binary, y_binary)\n",
        "auc_binary_l1 = roc_auc_score(y_binary, clf_binary_l1.predict_proba(X_binary)[:, 1])\n",
        "print(f\"Binary Classification (L1 Penalty) - AUC: {auc_binary_l1:.4f}\")\n",
        "\n",
        "# Testing L2 Penalty with increased iterations\n",
        "clf_binary_l2 = LogisticRegression(solver=\"liblinear\", penalty=\"l2\", max_iter=1000).fit(X_binary, y_binary)\n",
        "auc_binary_l2 = roc_auc_score(y_binary, clf_binary_l2.predict_proba(X_binary)[:, 1])\n",
        "print(f\"Binary Classification (L2 Penalty) - AUC: {auc_binary_l2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9MfyuxqL-8O",
        "outputId": "629f33f1-88a4-44d6-d8e6-d35b285a4a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Classification (L1 Penalty) - AUC: 0.9951\n",
            "Binary Classification (L2 Penalty) - AUC: 0.9948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "- L1 Penalty - AUC: 0.9951\n",
        "- L2 Penalty - AUC: 0.9948\n",
        "\n",
        "The results show that both penalties achieved high AUC values, with l1 slightly outperforming the baseline, while l2 closely matched it. The slight improvement with l1 suggests its feature selection properties may enhance model generalisation, while l2 provides stable and reliable performance."
      ],
      "metadata": {
        "id": "cYx-5ydyPSlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train-Test Split**\n",
        "\n",
        "In this section, I experiment with different train-test splits to observe how varying the proportion of data used for training and testing affects the model's performance. By iterating over splits of 20%, 50%, and 70% for testing, I aim to understand the impact of data distribution on prediction accuracy."
      ],
      "metadata": {
        "id": "ln6ZuLYSSHXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = [0.2, 0.5, 0.7]\n",
        "\n",
        "for test_size in splits:\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Train SVM model\n",
        "    svr = svm.SVR(kernel=\"linear\")\n",
        "    svr.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svr.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Print results for this split\n",
        "    print(f\"Test Size: {test_size}\")\n",
        "    print(f\"R² Score: {r2:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS9uB_lPTrfx",
        "outputId": "d2007fe1-5374-4a5b-ee13-4a39245a522a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Size: 0.2\n",
            "R² Score: 0.9481\n",
            "\n",
            "Test Size: 0.5\n",
            "R² Score: 0.9392\n",
            "\n",
            "Test Size: 0.7\n",
            "R² Score: 0.9218\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "The experiment demonstrates that as the test size increases, the R² score slightly decreases. With a smaller test size, the model achieves the highest R² score of 0.9481, indicating better performance when more data is available for training. As the test size grows to 50% and 70%, the R² scores decrease to 0.9392 and 0.9218, respectively, reflecting the trade-off between training data size and the model's ability to generalise to unseen data.\n",
        "\n",
        "It is worth noting that the difference between the R² score of 0.9392 at test size 0.5 and the baseline R² (which was also calculated based on a test size of 0.5) is likely due to the randomness introduced during the train-test split."
      ],
      "metadata": {
        "id": "n-rUtRapVQro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Noise levels**\n",
        "\n",
        "In this section, I experiment with different noise levels added to the dataset to evaluate their impact on the model's performance. By introducing varying amounts of random noise to the features and training an SVM model, I measure the resulting R² scores to observe how well the model generalises under noisy conditions. This analysis helps assess the model's robustness to data distortion.\n"
      ],
      "metadata": {
        "id": "fegXgkuQXKe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with different noise levels\n",
        "noise_levels = [0, 0.5, 1.0]\n",
        "test_size = 0.5\n",
        "\n",
        "for noise in noise_levels:\n",
        "    # Add noise to the features\n",
        "    random_state = np.random.RandomState(42)\n",
        "    X_noisy = X + noise * random_state.normal(size=X.shape)\n",
        "\n",
        "    # Split the noisy data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_noisy, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    # Train SVM model\n",
        "    svr = svm.SVR(kernel=\"linear\")\n",
        "    svr.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = svr.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Noise Level: {noise}\")\n",
        "    print(f\"R² Score: {r2:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9X9otctXLLI",
        "outputId": "b7649a9b-6294-4862-dd94-690dace52920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noise Level: 0\n",
            "R² Score: 0.9392\n",
            "\n",
            "Noise Level: 0.5\n",
            "R² Score: 0.8293\n",
            "\n",
            "Noise Level: 1.0\n",
            "R² Score: 0.6389\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "The experiment shows that increasing noise levels in the dataset reduces the model's performance, as seen by declining R² scores. Without noise, the model achieves the highest R², indicating strong accuracy, while higher noise levels make it harder for the model to capture meaningful patterns, highlighting its sensitivity to data distortion."
      ],
      "metadata": {
        "id": "z5uQ3v-1YA8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "These experiments show how train-test split, regularisation, solver choice, and noise levels affect model performance. The results highlight the importance of fine-tuning parameters and ensuring data quality for achieving reliable and generalisable models."
      ],
      "metadata": {
        "id": "wMfThbRSYHDL"
      }
    }
  ]
}